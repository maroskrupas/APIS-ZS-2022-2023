{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](Images/cisco.png)\n",
    "\n",
    "# Lab - Smile Detection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "The objective of this lab is to use machine learning to detect whether a person is smiling when given a picture or a video.\n",
    "\n",
    "<b>\n",
    "* Part 1: Detect Faces\n",
    "* Part 2: Data Processing: Face Standardization\n",
    "* Part 3: Load a Pre-trained Moel\n",
    "* Part 4: Real-time Smile Detection\n",
    "</b>\n",
    "\n",
    "### Scenario/Background\n",
    "\n",
    "In this lab, you will get to work on a complete image processing pipeline. You will use a machine learning model both to detect a face inside of an image and to understand whether the detected face is smiling. You will then build a real-time smile detector, that will be used to take pictures when a person is smiling.\n",
    "\n",
    "### Required Resources\n",
    "* 1 PC with Internet access\n",
    "* Raspberry Pi version 2 or higher\n",
    "* Python libraries: picamera, cv2, time, matplotlib, IPython, numpy, scipy, sklearn\n",
    "* Datafiles: smile.jpg, nosmile.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Detect Faces\n",
    "\n",
    "There are two steps that involve machine learning:\n",
    "* Supervised learning to detect a face\n",
    "* Model evaluation to d etermine if the face is smiling\n",
    "\n",
    "Training a model means teaching a machine how to perform a task, but rather than explicitly programming it with a predefined series of steps, the machine will have access to a dataset and will adapt its parameters to match the information provided in the training set. \n",
    "\n",
    "##### Supervised learning\n",
    "\n",
    "The scenario presented in this lab belongs to the supervised learning set up. In supervised learning, the computer has access to a training set, such as a collection of pictures with faces. Each of these faces is a data point associated with a label, such as \"smiling\" or \"not smiling\".\n",
    "\n",
    "This lab is presented with a binary classification task. There are two classes: one containing all the pictures where there is a smiling face and the other group containing a face that is not smiling. The goal of the model is to assign a data point to one of the two classes: smiling and not smiling.\n",
    "\n",
    "How does the computer know how to classify a data point? The computer will observe the relationship between training points and labels, and will try to infer the rules that connect the two, so that when a new, unseen data point arrives, it will be possible to classify it following the same rules.\n",
    "\n",
    "In traditional programming, a computer was given an algorithm, a dataset, and it produced an output:\n",
    "\n",
    "![Title](Images/traditional.png)\n",
    "\n",
    "In supervised machine learning, a computer is given input data and output data, and it produces an algorithm. \n",
    "\n",
    "![Title](Images/ML.png)\n",
    "\n",
    "The quality of the algorithm depends on the quality of the given input. It is crucial that the input data is a good representation of the model being considered.\n",
    "\n",
    "In this lab, a pre-trained model is provided for face detection and smile recognition. This pre-trained model has already learned the relationship between input faces and output labels. \n",
    "\n",
    "This is a sample of the faces used to train the model:\n",
    "\n",
    "![title](Images/olivetti_sample.png)\n",
    "\n",
    "They come from one of the datasets that can be found in sklearn (see http://scikit-learn.org/stable/datasets/olivetti_faces.html). The Olivetti Faces dataset is typically used for identity recognition. This dataset has ten different images of each of the 40 distinct subjects. In this lab, the original target variables were replaced with binary labels, a similing face or a not similing face.\n",
    "\n",
    "##### Model evaluation\n",
    "\n",
    "The dataset has been divided into a training and a test set. 75% of the data is in the training set and 25% is in the test set. It is good practice to do this to evaluate the model performance, and to avoid having a super complicated model that fits the training data perfectly but does not generalize, known as overfitting.\n",
    "\n",
    "The first task, given an image, will be to identify the face(s) inside of it: to do so, you will use the OpenCV pipeline for face detection. With the same library, it is possible to build real time applications for mobile phones, for example.\n",
    "\n",
    "To understand whether the detected face is smiling or not, some pre-processing is needed. The picture will have to be transformed to be compatible with the data accepted by the model. In this case, each face will have to be of dimension 64x64 pixels, and be greyscale.\n",
    "\n",
    "An offline pipeline is implemented to see how the model works on two test images. Then, the same steps will be embedded in an online pipeline, so that the smile detector will work in real time, acquiring the images from the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OpenCV  <img src=\"Images/opencv_logo.png\" align=\"left\", width=\"20\">   \n",
    "\n",
    "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in commercial products. It has C++, C, Python and Java interfaces, and supports Windows, Linux, Mac OS, iOS, and Android.\n",
    "\n",
    "The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms, such as facial detection and recognition. The library is used extensively in companies, research groups, and by governmental bodies. [source](http://opencv.org/about.html)\n",
    "\n",
    "### OpenCV-Python\n",
    "\n",
    "Compared to other languages like C/C++, Python is slower. But another important feature of Python is that it can be easily extended with C/C++. This feature helps us to write computationally intensive codes in C/C++ and create a Python wrapper for it so that we can use these wrappers as Python modules. This gives us two advantages: first, our code is as fast as original C/C++ code because it is the actual C++ code working in background, and second, it is very easy to code in Python. This is how OpenCV-Python works, it is a Python wrapper around original C++ implementation. [source](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_intro/py_intro.html#intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will import the module `cv2`, the Python version of `OpenCV`, and other libraries for loading and viewing the image data.\n",
    "\n",
    "For more information about reading and writing images and video, click [here](http://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 1\n",
    "# Importing cv2\n",
    "import cv2\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read and view test data.\n",
    "\n",
    "The two provided test images are `smile.jpg` and `nosmile.jpg`. It is possible to read and store them in variables through the function `cv2.imread()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the two test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 2\n",
    "smile = cv2.imread('Images/smile.jpg')\n",
    "no_smile = cv2.imread('Images/nosmile.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Visualize the two test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 3\n",
    "# visualize the two test images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "clear_output()\n",
    "ax[0].imshow(cv2.cvtColor(smile, cv2.COLOR_BGR2GRAY), cmap='gray')\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Smile')\n",
    "ax[1].imshow(cv2.cvtColor(no_smile, cv2.COLOR_BGR2GRAY), cmap='gray')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('No smile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Face detection\n",
    "\n",
    "The provided model accepts faces as input, so it is necessary to find the face(s) inside the images. OpenCV provides a pipeline to detect faces using a cascade classifier trained on haar-like features. \n",
    "\n",
    "Haar-like features are filters that highlight different structures inside of an image. For instance, they will spot edges, lines, corners etc. A cascade classifier is a sequence of classifiers that operate with features at different levels. The idea is that each image is decomposed with respect to these features. The model already knows which of these features are included in a face image. When the relevant group of features is spotted by the model, a face is returned. \n",
    "\n",
    "In practice, the face detection pipeline works as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Select the file containing the haar features needed to detect faces inside an image and initialize a cascade classifier in OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 4\n",
    "# path to the file containing the features that the openCv pipeline will look for in the frame\n",
    "cascadePath = \"Data/haarcascade_frontalface_default.xml\"\n",
    "# initialize a model for detecting whether a certain portion of an image contains a face\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Convert the frame to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 5\n",
    "gray_smile = cv2.cvtColor(smile, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the OpenCv classifier to detect the faces inside the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 6\n",
    "# openCv algorithm to scan the image looking for haar features\n",
    "faces = faceCascade.detectMultiScale(\n",
    "        gray_smile,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=6,\n",
    "        minSize=(100, 100),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "# output of the classifier\n",
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Visualize the outcome of the algorithm by drawing a green rectangle around the face detected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 7\n",
    "# draw a rectangle\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# visualize the detected face\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(gray_smile, cmap='gray')\n",
    "\n",
    "# iterate over all the detected faces\n",
    "for face in faces:\n",
    "    # retrieve the coordinates of the position of the current face, and its size    \n",
    "    (x_smile, y_smile, w_smile, h_smile) = face\n",
    "    # draw a rectangle where the face is detected    \n",
    "    ax.add_artist(Rectangle((x_smile, y_smile), w_smile, h_smile, fill=False, lw=3, color='green'))\n",
    "\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat on the other test image to detect the face using OpenCV classifier and convert the image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 8\n",
    "# convert the image to grayscale\n",
    "# gray_nosmile = ...\n",
    "\n",
    "# openCv algorithm to scan the image looking for haar features\n",
    "# faces = ...\n",
    "\n",
    "# output of the classifier\n",
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Visualize the outcome of the algorithm by drawing a red rectangle around the face detected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 9\n",
    "# visualize the detected face\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(gray_nosmile, cmap='gray')\n",
    "\n",
    "# iterate over all the detected faces\n",
    "for face in faces:\n",
    "    # retrieve the coordinates of the position of the current face, and its size\n",
    "    #(x_nosmile, y_nosmile, w_nosmile, h_nosmile) = ...\n",
    "    \n",
    "    # draw a rectangle where the face is detected\n",
    "    #...\n",
    "    \n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Processing: Face Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part, you determined the location and the size of the face. \n",
    "It is necessary to extract it from the image and transform it to the correct format expected by the model. This part of the pipeline is data pre-processing, and it is usually crucial to get good performances.\n",
    "\n",
    "Note that the dimensions used to extract the face are not exactly the same of the detected face, but there are two coefficients to stretch the image. This is because it helps in making the faces more similar to the ones composing the training set over which the model was trained, to make the classification task easier or even possible. \n",
    "\n",
    "This is a sample of the faces used to train the model:\n",
    "\n",
    "![title](Images/olivetti_sample.png)\n",
    "\n",
    "The model will work well if the faces are similar to the ones used for training, up to a certain degree, in terms of position of the different facial features. \n",
    "\n",
    "This is a task that is very specific for this case, however it is important to think about how similar to the training data the new data points are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Select the portion that will be used to classify whether the face is smiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 10\n",
    "# select only the face portion from the smile test image\n",
    "face_smile = gray_smile[y_smile:y_smile+h_smile, x_smile:x_smile+w_smile]\n",
    "\n",
    "# The cropping coefficient for determining the size of the face\n",
    "c1 = 0.2\n",
    "\n",
    "# calculate how to crop the face\n",
    "# vertical dimension\n",
    "v_cut = int(c1 * w_smile)\n",
    "# horizontal dimension\n",
    "h_cut = int(c1 * h_smile)\n",
    "\n",
    "# select only the face portion from the smile test image\n",
    "cut_face_smile = gray_smile[y_smile+v_cut:y_smile+h_smile, \n",
    "                      x_smile+h_cut:x_smile-h_cut+w_smile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Repeat for the other test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 11\n",
    "# select only the face portion from the non-smile test image\n",
    "# face nosmile = ...\n",
    "\n",
    "# The cropping coefficient for determining the size of the face\n",
    "c2 = 0.2\n",
    "\n",
    "# calculate how to crop the face \n",
    "# vertical dimension\n",
    "v_cut = int(c2 * w_nosmile)\n",
    "# horizontal dimension\n",
    "h_cut = int(c2 * h_nosmile)\n",
    "\n",
    "# select only the face portion from the non smile test image\n",
    "# cut_face_nosmile = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Visualize the original detected faces and the distorted ones, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 12\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "ax[0][0].imshow(face_smile, cmap='gray')\n",
    "ax[0][0].axis('off')\n",
    "ax[0][0].set_title('Original')\n",
    "\n",
    "ax[0][1].imshow(cut_face_smile, cmap='gray')\n",
    "ax[0][1].axis('off')\n",
    "ax[0][1].set_title('Cropped')\n",
    "\n",
    "ax[1][0].imshow(face_nosmile, cmap='gray')\n",
    "ax[1][0].axis('off')\n",
    "ax[1][1].imshow(cut_face_nosmile, cmap='gray')\n",
    "ax[1][1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would the two coefficients (cv and ch) affect the detected faces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>*Type your answer here.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) A modification is needed, so that the data points are compatible with the pre-trained model The faces must be 64x64 pixels. This means that the aspect ratio will change. It is possible to achieve this result using the method `zoom()` from `scipy.ndimage` (see http://scipy.github.io/devdocs/generated/scipy.ndimage.zoom.html#scipy.ndimage.zoom).\n",
    "\n",
    "Modify the dimensions of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 13\n",
    "# import the numpy and scipy libraries to zoom the images\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# transform the stretched smiling face so that it has 64x64 pixels\n",
    "standardized_face_smile = zoom(cut_face_smile, (64. / cut_face_smile.shape[0], \n",
    "                                           64. / cut_face_smile.shape[1])).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) The numeric values on which the model is trained range from 0 to 1. It is possible to achieve this normalizing the data, dividing by 255, the maximum value a pixel can take.\n",
    "\n",
    "Normalize the zoomed face by dividing by the maximum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 14\n",
    "# normalize the image so that its values are between 0 and 1\n",
    "standardized_face_smile /= float(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Now repeat for the other test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 15\n",
    "# transform the stretched non smiling face so that it has 64x64 pixels\n",
    "# standardized_face_nosmile = zoom(...)\n",
    "\n",
    "# normalize the image so that its values are between 0 and 1\n",
    "# standardized_face_nosmile /= ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Visualize the two standardized faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 16\n",
    "plt.subplot(121)\n",
    "plt.imshow(standardized_face_smile[:, :], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(standardized_face_nosmile[:, :], cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the two faces are slightly stretched. This makes them more similar to the original training images:\n",
    "\n",
    "![Title](Images/olivetti_big.png)\n",
    "\n",
    "The test faces are now in the right format to be processed by the pre-trained module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading a Pre-trained Model\n",
    "\n",
    "Now the data is in the correct format. It is possible to load the pre-trained model, that is contained in a `pickle` file. `Pickle` is the file format to save serialized Python objects (a machine learning module in this case). To know more, see https://docs.python.org/2/library/pickle.html.\n",
    "\n",
    "The model was trained with a Support Vector Machine (SVM), a supervised classification algorithm that looks for the optimal decision boundary to separate the different classes.\n",
    "The model that implements the SVM method is `sklearn`. \n",
    "\n",
    "`sklearn` is one of the most used Python libraries for machine learning. It implements a wide number of methods, and offers functionalities for pre-processing, cross validation, etc. You can have a look at documentation, tutorials, and more here: http://scikit-learn.org/stable/documentation.html.\n",
    "\n",
    "To learn more about SVMs in sklearn, see http://scikit-learn.org/stable/modules/svm.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the provided pre-trained SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 17\n",
    "import pickle as pkl\n",
    "\n",
    "# load LR model\n",
    "with open('Data/pl-app_2.0.0-support_vector_machines.pkl', 'rb') as f:\n",
    "    classifier = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the classifier to predict whether the person in the first test picture is smiling or not.\n",
    "\n",
    "Each classifier in sklearn has a method `predict()`, that takes as input a data point in the correct format and returns the answer of the classifier. The model is a binary classifier, and will return a 0 if the face is not smiling, and a 1 if the face is smiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 18\n",
    "pred = classifier.predict((standardized_face_smile.ravel()).reshape(1, -1))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) To make everything a bit more user friendly, it is possible to create a dictionary that maps a 0 to \"NOT SMILING\" and a 1 to \"SMILING\". It will be useful to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 19\n",
    "answers = {0: 'NOT SMILING.', 1:'SMILING.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Visualize the test image and the answer given by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 20\n",
    "plt.imshow(gray_smile[:, :], cmap='gray')\n",
    "plt.axis('off')\n",
    "print('The person in this picture is', answers[pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat everything with the other test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 21\n",
    "# pred = classifier.predict(...)\n",
    "\n",
    "plt.imshow(gray_nosmile[:, :], cmap='gray')\n",
    "plt.axis('off')\n",
    "# Print a message with the result of the smile detection\n",
    "# print ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Real Time Smile Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal of this lab is to take a selfie of smiling people. In this part, you will set up the Picamera, take pictures of smiling people, and save the frame to the folder `selfies`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Import the Python script, `smile_utils`.\n",
    "\n",
    "The Python script [smile_utils.py](smile_utils.py) implements all the actions of the previous steps (face detection, face standardization and smile detection) in functions that can be called in an online loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 22\n",
    "import smile_utils as su\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Initilize the cropping coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 23\n",
    "# remember the cropping coefficients? Here is an initialization for them.\n",
    "c1 = 0.1\n",
    "c2 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Import the library for use with the Pi camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 24\n",
    "# Setting up the camera\n",
    "from picamera import PiCamera\n",
    "from picamera.array import PiRGBArray\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Define the function, `showFrame`, to display the video output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 25\n",
    "def showFrame(frame):\n",
    "    plt.axis('off')\n",
    "    plt.title('Video Output')\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Online smile detection\n",
    "\n",
    "a) Verify camera connection and smile detection.\n",
    "\n",
    "The following piece of code puts together everything shown in this lab, from camera connection to smile detection, in order for it to work in real time.\n",
    "\n",
    "Wait about 1 min. Click the Stop button to release the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code Cell 26\n",
    "try:\n",
    "    camera.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# initialize the camera and acquire a reference to the raw camera capture\n",
    "camera = PiCamera()\n",
    "# Use these two lines to reorient your image if necessary.\n",
    "#camera.vflip = True\n",
    "#camera.hflip = True\n",
    "camera.resolution = (640, 480)\n",
    "camera.framerate = 3\n",
    "# allow the camera to warmup\n",
    "time.sleep(1)\n",
    "try:\n",
    "    # looping to get a video from the camera\n",
    "    while True:\n",
    "        rawCapture = PiRGBArray(camera)\n",
    "\n",
    "        # acquire an image from the camera\n",
    "        camera.capture(rawCapture, format=\"rgb\")\n",
    "        frame = rawCapture.array\n",
    "        # creates a copy of the frame to be able to draw on top of it\n",
    "        new_frame = frame.copy()\n",
    "\n",
    "        # detect faces\n",
    "        gray, faces = su.find_face(frame)\n",
    "    \n",
    "        # keeps track of how many faces are detected inside a frame\n",
    "        face_index = 0\n",
    "        \n",
    "        # loop over all the detected faces\n",
    "        for face in faces:\n",
    "            \n",
    "            # get the coordinates/size of the current face\n",
    "            (x, y, w, h) = face\n",
    "\n",
    "            if w > 100:\n",
    "            \n",
    "                # standardize the face\n",
    "                standardized_face = su.standardize(gray, face, (c1, c2)) #(0.075, 0.05)\n",
    "\n",
    "                # classify the face\n",
    "                result = su.is_smiling(standardized_face, classifier)\n",
    "\n",
    "                # draw extracted face in the top right corner - to have an idea \n",
    "                # of what the model is trying to predict\n",
    "                new_frame[face_index * 64: (face_index + 1) * 64, :64, :] = cv2.cvtColor(standardized_face*255, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "                # depending on the result of the classification, pick red or green as a color\n",
    "                if result == 1:\n",
    "                    color = (0, 255, 0)\n",
    "                else:\n",
    "                    color = (255, 0, 0)\n",
    "\n",
    "                # increment counter\n",
    "                face_index += 1\n",
    "                # draw rectangle around face - green if smiling, red if not smiling\n",
    "                cv2.rectangle(new_frame, (x, y), (x+w, y+h), color, 2)   \n",
    "\n",
    "        # Display the new frame\n",
    "        showFrame(new_frame)\n",
    "       \n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    # Click Stop (Black square in the menu) to release the video resource\n",
    "    \n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")\n",
    "    camera.close()\n",
    "    showFrame(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) After you have verified that the Pi camera module and the smile detector are functioning, you can take a selfie of the smiling people.\n",
    "\n",
    "If a person smiles for more than a given number of frames, the following piece of code saves the frame to the folder `selfies`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code Cell 27\n",
    "try:\n",
    "    camera.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# initialize the camera and acquire a reference to the raw camera capture\n",
    "camera = PiCamera()\n",
    "# Use these two lines to reorient your image if necessary\n",
    "#camera.vflip = True\n",
    "#camera.hflip = True\n",
    "camera.resolution = (640, 480)\n",
    "camera.framerate = 3\n",
    "# allow the camera to warmup\n",
    "time.sleep(1)\n",
    "k=0\n",
    "num_selfie=0\n",
    "try:\n",
    "    # looping to get a video from the camera\n",
    "    while True:\n",
    "        rawCapture = PiRGBArray(camera)\n",
    "\n",
    "        # grab an image from the camera\n",
    "        camera.capture(rawCapture, format=\"rgb\")\n",
    "        frame = rawCapture.array\n",
    "        # creates a copy of the frame in order to be able to draw on top of it\n",
    "        new_frame = frame.copy()\n",
    "\n",
    "        # detect faces\n",
    "        gray, faces = su.find_face(frame)\n",
    "    \n",
    "        # keeps track of how many faces are detected inside a frame\n",
    "        face_index = 0\n",
    "        \n",
    "        # loop over all the detected faces\n",
    "        for face in faces:\n",
    "        \n",
    "            (x, y, w, h) = face\n",
    "            if w > 100:\n",
    "            \n",
    "                # standardize the face\n",
    "                standardized_face = su.standardize(gray, face, (c1, c2)) #(0.075, 0.05)\n",
    "\n",
    "                # classify the face\n",
    "                result = su.is_smiling(standardized_face, classifier)\n",
    "                \n",
    "                # depending on the result of the classification, pick red or green as a color\n",
    "                # also keeps track of the number of frames a person has been smiling\n",
    "                if result == 1:\n",
    "                    k+=1\n",
    "                    color = (0, 255, 0)\n",
    "                else:\n",
    "                    k=0\n",
    "                    color = (0, 0, 255)\n",
    "                # if a person has been smiling for more than 1 frames, take a selfie\n",
    "                if k > 1:\n",
    "                    plt.imsave('selfies/my_selfie_{}.png'.format(num_selfie), frame)\n",
    "                    num_selfie+=1\n",
    "                    new_frame = new_frame * 255\n",
    "                    k=0\n",
    "                # draw a rectangle on top of the face\n",
    "                cv2.rectangle(new_frame, (x, y), (x+w, y+h), color, 2)   \n",
    "\n",
    "                # increment counter\n",
    "                face_index += 1\n",
    "                # draw rectangle around face \n",
    "\n",
    "        # Display the resulting frame\n",
    "        showFrame(new_frame)\n",
    "\n",
    "except:\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")\n",
    "    camera.close()\n",
    "    showFrame(frame)\n",
    "    \n",
    "# Click Stop (Black square in the menu) to release the video resource after the image has been taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Navigate to the [selfies](selfies) folder to view your selfies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size='0.5'>&copy; 2017 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public.<font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
